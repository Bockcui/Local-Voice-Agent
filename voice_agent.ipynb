{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Voice Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add CosyVoice paths\n",
    "cosyvoice_base = r'C:\\Users\\MIDTOWER\\Documents\\Speech Synthesis Models\\CosyVoice'\n",
    "matcha_path = os.path.join(cosyvoice_base, 'third_party', 'Matcha-TTS')\n",
    "sys.path.insert(0, cosyvoice_base)\n",
    "sys.path.insert(0, matcha_path)\n",
    "\n",
    "# Other dependencies\n",
    "from fastapi import FastAPI, UploadFile, File\n",
    "from fastapi.responses import FileResponse\n",
    "import uvicorn\n",
    "import whisper\n",
    "import ollama\n",
    "from cosyvoice.cli.cosyvoice import CosyVoice2\n",
    "from cosyvoice.utils.file_utils import load_wav\n",
    "import torchaudio\n",
    "from datetime import datetime\n",
    "\n",
    "# Define components\n",
    "app = FastAPI(title=\"Voice Agent\", version=\"1.0.0\")\n",
    "asr_model = whisper.load_model(\"small\")\n",
    "model_path = os.path.join(cosyvoice_base, 'pretrained_models', 'CosyVoice2-0.5B')\n",
    "cosyvoice = CosyVoice2(model_path, load_jit=False, load_trt=False, load_vllm=False, fp16=False)\n",
    "prompt_audio_path = os.path.join(cosyvoice_base, 'asset', 'zero_shot_prompt.wav')\n",
    "prompt_speech_16k = load_wav(prompt_audio_path, 16000)\n",
    "\n",
    "# Conversation history\n",
    "conversation_history = []\n",
    "\n",
    "# Speech to text functionality\n",
    "def transcribe_audio(audio_bytes):\n",
    "    with open(\"temp.wav\", \"wb\") as f:\n",
    "        f.write(audio_bytes)\n",
    "    result = asr_model.transcribe(\"temp.wav\")\n",
    "\n",
    "    return result[\"text\"]\n",
    "\n",
    "# Prompting ollama functionality\n",
    "def generate_response(user_text):\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_text})\n",
    "    recent_history = conversation_history[-10:]\n",
    "    response = ollama.chat(model='llama2', messages=recent_history)\n",
    "    bot_response = response['message']['content']\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": bot_response})\n",
    "    \n",
    "    return bot_response\n",
    "\n",
    "# Speech synthesis functionality\n",
    "def synthesize_speech(text, output_path=None):\n",
    "    if output_path is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")[:-3]\n",
    "        output_path = f\"response_{timestamp}.wav\"\n",
    "\n",
    "    for i, j in enumerate(cosyvoice.inference_zero_shot(\n",
    "        text,\n",
    "        \"Remind yourself overconfidence is a slow and insidious killer.\",\n",
    "        prompt_speech_16k,\n",
    "        stream=False\n",
    "    )):\n",
    "        torchaudio.save(output_path, j['tts_speech'], cosyvoice.sample_rate)\n",
    "        print(f\"Saved audio: {output_path}, size: {os.path.getsize(output_path)} bytes\")\n",
    "        break\n",
    "    \n",
    "    if not os.path.exists(output_path):\n",
    "        raise Exception(f\"Output file not created: {output_path}\")\n",
    "    \n",
    "    if os.path.getsize(output_path) == 0:\n",
    "        raise Exception(f\"Output file is empty: {output_path}\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# API definition\n",
    "@app.post(\"/chat/\")\n",
    "async def chat_endpoint(file: UploadFile = File(...)):\n",
    "    try:\n",
    "        # Step 1: Receive and transcribe audio\n",
    "        audio_bytes = await file.read()\n",
    "        user_text = transcribe_audio(audio_bytes)\n",
    "        print(f\"User said: {user_text}\")\n",
    "        \n",
    "        # Step 2: Generate response with LLM\n",
    "        bot_text = generate_response(user_text)\n",
    "        print(f\"Bot responds: {bot_text}\")\n",
    "        \n",
    "        # Step 3: Convert response to speech\n",
    "        audio_path = synthesize_speech(bot_text)\n",
    "        print(f\"Audio generated: {audio_path}\")\n",
    "        \n",
    "        # Step 4: Return audio response\n",
    "        return FileResponse(audio_path, media_type=\"audio/wav\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "def run_server():\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "server_thread = threading.Thread(target=run_server, daemon=True)\n",
    "server_thread.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
